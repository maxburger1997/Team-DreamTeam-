{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2a4f57f",
   "metadata": {
    "id": "e2a4f57f"
   },
   "source": [
    "# Machine Learning Project 2022: Plankton\n",
    "\n",
    "### Authors:\n",
    "- Bram Fresen\n",
    "- Bram Huis\n",
    "- Max Burger\n",
    "- Moos Middelkoop\n",
    "\n",
    "For the Machine Learning Project to finish off the minor Artificial Intelligence, we chose to tackle the plankton problem, originally uploaded as the United States national data science bowl in december 2014. For this problem, the goal is to classify microscopic images of particles in water as one of 121 different classes of plankton. The dataset is 30.000 images large, with varying sizes. The dataset is also imbalanced.\n",
    "\n",
    "In order to solve this problem we will make use of a Convolutional Neural Network using the tensorflow library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb144121",
   "metadata": {
    "id": "fb144121"
   },
   "source": [
    "## Import libraries\n",
    "\n",
    "Firstly, we will import the needed libraries, and check if we are running on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc8cd45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccc8cd45",
    "outputId": "5ceaf3a6-89a3-4132-b874-2056369f237b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from tensorflow.keras import layers, models, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d80ec50",
   "metadata": {
    "id": "0d80ec50"
   },
   "source": [
    "## Loading the training data and training labels\n",
    "\n",
    "We use the cv2 library to load the training images (which are .jpg files), and turn them into arrays. This piece of code was found online:\n",
    "\n",
    "https://stackoverflow.com/questions/30230592/loading-all-images-using-imread-from-a-given-folder\n",
    "https://drive.google.com/file/d/1hAaPzDMVEZ8X1tfRS2ieFEqi0R7Ww7uL/view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851be41",
   "metadata": {
    "id": "c851be41"
   },
   "source": [
    "### Training data\n",
    "\n",
    "Training data is sorted into folders by class, this next piece of code reads in the training data, puts it in an array, and constructs an array for the classes by using the names of the folders. At the end, this array for the labels is turned into a one-hot matrix, so that tensorflow can work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024ae544",
   "metadata": {
    "id": "024ae544"
   },
   "outputs": [],
   "source": [
    "# Create empty lists for the not resized training data, the labels (not one hot encoded yet) and the class sizes\n",
    "train_data = []\n",
    "train_labels = []\n",
    "class_size_list = []\n",
    "\n",
    "# Create a folder path of the different categories\n",
    "folder_train = 'data/train'\n",
    "\n",
    "# Loop through the index (for the one hot matrix) and the categories\n",
    "for number, categories in enumerate(os.listdir(folder_train)):\n",
    "    class_size = 0\n",
    "    \n",
    "    # Loop through the images , add 1 to the class size, read the images in in and add them to a list, \n",
    "    #also add the index 'number' to a list for the one hot matrix\n",
    "    for image in os.listdir(f'data/train/{categories}'):\n",
    "        class_size += 1\n",
    "        train_labels.append(number)\n",
    "        img = cv2.imread(os.path.join(f'data/train/{categories}', image))\n",
    "        train_data.append(img)\n",
    "    \n",
    "    # Append the size of the class to the class size list, in order to check the class sizes later, this way we\n",
    "    # can ananlyze the degree of class imbalance\n",
    "    class_size_list.append(class_size)\n",
    "\n",
    "# Create a one hot matrix from the train labels\n",
    "train_labels_one_hot = tf.keras.utils.to_categorical(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1615b0",
   "metadata": {},
   "source": [
    "Analyzing degree of class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6deaa34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[889, 13, 71, 49, 16, 532, 242, 696, 393, 170, 815, 1934, 694, 77, 681, 173, 96, 178, 63, 286, 106, 49, 87, 30, 899, 1189, 24, 201, 113, 42, 53, 38, 55, 363, 394, 914, 519, 500, 36, 92, 80, 88, 385, 536, 96, 27, 14, 136, 38, 511, 10, 31, 85, 114, 64, 16, 10, 127, 75, 35, 229, 9, 19, 132, 23, 336, 12, 190, 412, 274, 150, 76, 703, 123, 43, 56, 14, 61, 14, 24, 141, 131, 108, 372, 625, 1172, 113, 108, 13, 65, 287, 158, 52, 49, 153, 174, 212, 135, 483, 179, 57, 247, 29, 30, 128, 21, 24, 38, 708, 54, 1979, 678, 29, 439, 417, 352, 236, 73, 317, 175, 425]\n",
      "\n",
      "Biggest class: 1979\n",
      "\n",
      "Smallest class: 9\n"
     ]
    }
   ],
   "source": [
    "print(class_size_list)\n",
    "print()\n",
    "print(f'Biggest class: {max(class_size_list)}')\n",
    "print()\n",
    "print(f'Smallest class: {min(class_size_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a0704a",
   "metadata": {
    "id": "89a0704a"
   },
   "source": [
    "### Deleting unnecessary channels\n",
    "\n",
    "There are 3 channels in each of the images. The cell below shows that each of the channels have the same values (test 1 and 2). Test 3 and 4 are here to show that the 'np.array_equal' function works; the values from different images return a false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9048ef",
   "metadata": {
    "id": "2b9048ef",
    "outputId": "8b5b04c2-5173-4bed-919f-2c60161f030c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test 1: True, Test 2: True\n",
      " Test 3: False, Test 4: False\n"
     ]
    }
   ],
   "source": [
    "x = train_data[0]\n",
    "y = train_data[1]\n",
    "\n",
    "a0 = x[:, :, 0]\n",
    "b0 = x[:, :, 1]\n",
    "c0 = x[:, :, 2]\n",
    "\n",
    "a1 = y[:, :, 0]\n",
    "b1 = y[:, :, 1]\n",
    "c1 = y[:, :, 2]\n",
    "\n",
    "test_1 = np.array_equal(a0, b0)\n",
    "test_2 = np.array_equal(b0, c0)\n",
    "\n",
    "test_3 = np.array_equal(a0, a1)\n",
    "test_4 = np.array_equal(b0, c1)\n",
    "\n",
    "\n",
    "print(f' Test 1: {test_1}, Test 2: {test_2}')\n",
    "print(f' Test 3: {test_3}, Test 4: {test_4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d6cb5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "qaI7P5qy9L3Q",
    "outputId": "7552739a-b447-4b3d-ea37-22a6185e540f"
   },
   "source": [
    "We only proceed with one of the three channels, to eliminate redundant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "qaI7P5qy9L3Q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "qaI7P5qy9L3Q",
    "outputId": "7552739a-b447-4b3d-ea37-22a6185e540f"
   },
   "outputs": [],
   "source": [
    "for i, image in enumerate(train_data):\n",
    "      train_data[i] = image[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7d208c",
   "metadata": {
    "id": "1d7d208c"
   },
   "source": [
    "# Resize input images\n",
    "\n",
    "Because all images are differnt sizes, it is necessary to resize all input data to the same size, in order to make tensorflow be able to work with the data. The first cell below analyzes the sizes of the data, and the second cell actually resizes, based on this analysis. Lastly, the data is converted into numpy arrays, so tensorflow will be able to work with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5838d255",
   "metadata": {
    "id": "5838d255",
    "outputId": "333e4faa-c45c-4ba3-e9a6-9943bfa2e55d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average dimensions: 66.66182093881856, 73.50728507383967\n",
      "Lowest first dimension image (21, 71)\n",
      "Lowest second dimension image (31, 49)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start with an infinitely large number\n",
    "value_1 = math.inf\n",
    "value_2 = math.inf\n",
    "sum_1 = 0\n",
    "sum_2 = 0\n",
    "count = 0\n",
    "\n",
    "# This checks for the lowest image size in the first and second dimension\n",
    "for image in train_data:\n",
    "    count+=1\n",
    "    sum_1 += image.shape[0]\n",
    "    sum_2 += image.shape[1]\n",
    "    if image.shape[0] < value_1:\n",
    "        \n",
    "        value_1 = image.shape[0]\n",
    "        hold_1 = image.shape\n",
    "    if image.shape[1] < value_2:\n",
    "        \n",
    "        value_2 = image.shape[0]\n",
    "        hold_2 = image.shape\n",
    "        \n",
    "sum_11 = sum_1 / count\n",
    "sum_22 = sum_2 / count\n",
    "\n",
    "print(f'Average dimensions: {sum_11}, {sum_22}')\n",
    "\n",
    "plt.imshow(train_data[0], cmap = 'gray')\n",
    "\n",
    "print(f'Lowest first dimension image {hold_1}')\n",
    "print(f'Lowest second dimension image {hold_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff1a2aa4",
   "metadata": {
    "id": "ff1a2aa4",
    "outputId": "150a5232-50fb-47e4-f6d5-18b8b06b0f37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an empty list for the training data\n",
    "train_data_resized = []\n",
    "\n",
    "# Loop through the images in the training data and resize them to the lowest shape in the dataset, \n",
    "# add a third dimension of 1 to the images and then append the image to the 'train_data_resized' list\n",
    "for image in train_data:\n",
    "    img = cv2.resize(image, dsize = (32, 32), interpolation = cv2.INTER_AREA)\n",
    "    img = np.expand_dims(img, axis = 2)\n",
    "    train_data_resized.append(img)\n",
    "\n",
    "# Test if the image is resized and show the image\n",
    "print(train_data_resized[0].shape)\n",
    "plt.imshow(train_data_resized[0], cmap = 'gray')\n",
    "\n",
    "#Split the data into 70% training and 30% validation\n",
    "im_train, im_val, lab_train, lab_val = train_test_split(train_data_resized, train_labels_one_hot, train_size=0.7, random_state=1265599650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cddaa21",
   "metadata": {
    "id": "9cddaa21",
    "outputId": "c51e8555-94fe-470f-9215-84e3d0d2ab4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21235, 32, 32, 1)\n",
      "(21235, 121)\n",
      "(9101, 32, 32, 1)\n",
      "(9101, 121)\n"
     ]
    }
   ],
   "source": [
    "# Convert the data to numpy arrays, so tensorflow can use them\n",
    "image_train = np.array(im_train)\n",
    "label_train = np.array(lab_train)\n",
    "image_val = np.array(im_val)\n",
    "label_val = np.array(lab_val)\n",
    "\n",
    "# Test if the shapes are correct\n",
    "print(image_train.shape)\n",
    "print(label_train.shape)\n",
    "print(image_val.shape)\n",
    "print(label_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15024880",
   "metadata": {
    "id": "15024880"
   },
   "source": [
    "## Convolutional network\n",
    "\n",
    "We use the function 'train_and_evaluate' which, obviously, trains our model and then evaluates the trained model on the validation data. This function was reused from the CIFAR-assignment from module 6 of ML2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ce50c7",
   "metadata": {},
   "source": [
    "Y_ints line: https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b49593d",
   "metadata": {
    "id": "3b49593d"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_x, train_y, val_x, val_y, preprocess={}, epochs=20, augment={}):\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    train_gen = preprocessing.image.ImageDataGenerator(**preprocess, **augment)\n",
    "    train_gen.fit(train_x) \n",
    "\n",
    "    val_gen = preprocessing.image.ImageDataGenerator(**preprocess)\n",
    "    val_gen.fit(train_x)\n",
    "    \n",
    "    y_ints = [y.argmax() for y in label_train]\n",
    "    class_weights = class_weight.compute_class_weight('balanced',classes = np.unique(y_ints), y = y_ints)\n",
    "    \n",
    "    history = model.fit(train_gen.flow(train_x, train_y), epochs=epochs, \n",
    "                        validation_data=val_gen.flow(val_x, val_y))\n",
    "\n",
    "    fig, axs = plt.subplots(1,2,figsize=(20,5)) \n",
    "\n",
    "    for i, metric in enumerate(['loss', 'accuracy']):\n",
    "        axs[i].plot(history.history[metric])\n",
    "        axs[i].plot(history.history['val_'+metric])\n",
    "        axs[i].legend(['training', 'validation'], loc='best')\n",
    "\n",
    "        axs[i].set_title('Model '+metric)\n",
    "        axs[i].set_ylabel(metric)\n",
    "        axs[i].set_xlabel('epoch')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Validation Accuracy: {model.evaluate(val_gen.flow(val_x, val_y))[1]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2820be",
   "metadata": {
    "id": "3c2820be"
   },
   "source": [
    "## The actual model\n",
    "\n",
    "We start with a very simple convolutional neural network, with 2 convolutional layers, both with pooling afterwards, and one dense layer. kernelsize, amount of filters, amount of nodes are specified in the code cell. This first version of the model gives us a validation accuracy of approximately 64%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a184d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 664 steps, validate for 285 steps\n",
      "Epoch 1/40\n",
      "664/664 [==============================] - 6s 9ms/step - loss: 3.7033 - accuracy: 0.1483 - val_loss: 2.8345 - val_accuracy: 0.2818\n",
      "Epoch 2/40\n",
      "664/664 [==============================] - 5s 8ms/step - loss: 2.6121 - accuracy: 0.3145 - val_loss: 2.2186 - val_accuracy: 0.3902\n",
      "Epoch 3/40\n",
      "664/664 [==============================] - 6s 8ms/step - loss: 2.2290 - accuracy: 0.3936 - val_loss: 1.9559 - val_accuracy: 0.4405\n",
      "Epoch 4/40\n",
      "664/664 [==============================] - 5s 8ms/step - loss: 2.0086 - accuracy: 0.4407 - val_loss: 1.8687 - val_accuracy: 0.4720\n",
      "Epoch 5/40\n",
      "664/664 [==============================] - 5s 8ms/step - loss: 8103.4726 - accuracy: 0.3479 - val_loss: 209.9637 - val_accuracy: 0.0732\n",
      "Epoch 6/40\n",
      "664/664 [==============================] - 5s 8ms/step - loss: 71.1527 - accuracy: 0.0508 - val_loss: 25.8736 - val_accuracy: 0.0527\n",
      "Epoch 7/40\n",
      "664/664 [==============================] - 5s 8ms/step - loss: 35.2763 - accuracy: 0.0567 - val_loss: 6.5300 - val_accuracy: 0.1276\n",
      "Epoch 8/40\n",
      "664/664 [==============================] - 5s 8ms/step - loss: 8711.6581 - accuracy: 0.0336 - val_loss: 13.6791 - val_accuracy: 0.1076\n",
      "Epoch 9/40\n",
      "664/664 [==============================] - 6s 8ms/step - loss: 38.3305 - accuracy: 0.0544 - val_loss: 7.1697 - val_accuracy: 0.1345\n",
      "Epoch 10/40\n",
      "664/664 [==============================] - 5s 8ms/step - loss: 31.0734 - accuracy: 0.0536 - val_loss: 20.4271 - val_accuracy: 0.0763\n",
      "Epoch 11/40\n",
      "664/664 [==============================] - 5s 8ms/step - loss: 14.6960 - accuracy: 0.0603 - val_loss: 6.5729 - val_accuracy: 0.0863\n",
      "Epoch 12/40\n",
      "664/664 [==============================] - 6s 8ms/step - loss: 33.8864 - accuracy: 0.0494 - val_loss: 6.9170 - val_accuracy: 0.0764\n",
      "Epoch 13/40\n",
      "664/664 [==============================] - 6s 8ms/step - loss: 120.7995 - accuracy: 0.0406 - val_loss: 4.8306 - val_accuracy: 0.0955\n",
      "Epoch 14/40\n",
      "664/664 [==============================] - 6s 9ms/step - loss: 6.3648 - accuracy: 0.0659 - val_loss: 4.1317 - val_accuracy: 0.1149\n",
      "Epoch 15/40\n",
      "664/664 [==============================] - 6s 8ms/step - loss: 4.9560 - accuracy: 0.0810 - val_loss: 5.5137 - val_accuracy: 0.0761\n",
      "Epoch 16/40\n",
      "141/664 [=====>........................] - ETA: 3s - loss: 5917.5084 - accuracy: 0.0601"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\bramh\\AppData\\Local\\Temp/ipykernel_6872/1427779340.py\", line 45, in <module>\n",
      "    train_and_evaluate(model_1, image_train, label_train, image_val, label_val, epochs = 40, augment = {'horizontal_flip':  True, 'vertical_flip': True, 'rotation_range': 0.15, 'zoom_range': 0.15})\n",
      "  File \"C:\\Users\\bramh\\AppData\\Local\\Temp/ipykernel_6872/331437690.py\", line 15, in train_and_evaluate\n",
      "    validation_data=val_gen.flow(val_x, val_y))\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\n",
      "    total_epochs=epochs)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\n",
      "    batch_outs = execution_function(iterator)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\n",
      "    distributed_function(input_fn))\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 568, in map_structure\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 568, in <listcomp>\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 130, in _non_none_constant_value\n",
      "    constant_value = tensor_util.constant_value(v)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\", line 822, in constant_value\n",
      "    return tensor.numpy()\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 942, in numpy\n",
      "    maybe_arr = self._numpy()  # pylint: disable=protected-access\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 908, in _numpy\n",
      "    return self._numpy_internal()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\bramh\\AppData\\Local\\Temp/ipykernel_6872/1427779340.py\", line 45, in <module>\n",
      "    train_and_evaluate(model_1, image_train, label_train, image_val, label_val, epochs = 40, augment = {'horizontal_flip':  True, 'vertical_flip': True, 'rotation_range': 0.15, 'zoom_range': 0.15})\n",
      "  File \"C:\\Users\\bramh\\AppData\\Local\\Temp/ipykernel_6872/331437690.py\", line 15, in train_and_evaluate\n",
      "    validation_data=val_gen.flow(val_x, val_y))\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\n",
      "    total_epochs=epochs)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\n",
      "    batch_outs = execution_function(iterator)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\n",
      "    distributed_function(input_fn))\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 568, in map_structure\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 568, in <listcomp>\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 130, in _non_none_constant_value\n",
      "    constant_value = tensor_util.constant_value(v)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\", line 822, in constant_value\n",
      "    return tensor.numpy()\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 942, in numpy\n",
      "    maybe_arr = self._numpy()  # pylint: disable=protected-access\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 908, in _numpy\n",
      "    return self._numpy_internal()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2067, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\bramh\\AppData\\Local\\Temp/ipykernel_6872/1427779340.py\", line 45, in <module>\n",
      "    train_and_evaluate(model_1, image_train, label_train, image_val, label_val, epochs = 40, augment = {'horizontal_flip':  True, 'vertical_flip': True, 'rotation_range': 0.15, 'zoom_range': 0.15})\n",
      "  File \"C:\\Users\\bramh\\AppData\\Local\\Temp/ipykernel_6872/331437690.py\", line 15, in train_and_evaluate\n",
      "    validation_data=val_gen.flow(val_x, val_y))\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\n",
      "    total_epochs=epochs)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\n",
      "    batch_outs = execution_function(iterator)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\n",
      "    distributed_function(input_fn))\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 568, in map_structure\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 568, in <listcomp>\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 130, in _non_none_constant_value\n",
      "    constant_value = tensor_util.constant_value(v)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\", line 822, in constant_value\n",
      "    return tensor.numpy()\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 942, in numpy\n",
      "    maybe_arr = self._numpy()  # pylint: disable=protected-access\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 908, in _numpy\n",
      "    return self._numpy_internal()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2067, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3173, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3383, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2067, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1143, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\bramh\\anaconda3\\envs\\progLab\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    }
   ],
   "source": [
    "first_layer_filters = 32\n",
    "second_layer_filters = 64\n",
    "third_layer_filters = 128\n",
    "fourth_layer_filters = 256\n",
    "\n",
    "kernelsize = (3,3)\n",
    "inputshape = (32,32, 1)\n",
    "first_hidden_layer_nodes = 256\n",
    "second_hidden_layer_nodes = 128\n",
    "output_nodes = 121\n",
    "# optimizer = 'adam'\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.00001)\n",
    "\n",
    "model_1 = models.Sequential()\n",
    "\n",
    "model_1.add(layers.Conv2D(first_layer_filters, kernelsize, activation = layers.LeakyReLU(alpha=0.1), padding = 'same', input_shape = inputshape))\n",
    "model_1.add(layers.Conv2D(first_layer_filters, kernelsize, activation = layers.LeakyReLU(alpha=0.1), padding = 'same'))\n",
    "model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model_1.add(layers.Conv2D(second_layer_filters, kernelsize, activation = layers.LeakyReLU(alpha=0.1), padding = 'same'))\n",
    "model_1.add(layers.Conv2D(second_layer_filters, kernelsize, activation = layers.LeakyReLU(alpha=0.1), padding = 'same'))\n",
    "model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model_1.add(layers.Conv2D(third_layer_filters, kernelsize, activation = layers.LeakyReLU(alpha=0.1), padding = 'same'))\n",
    "model_1.add(layers.Conv2D(third_layer_filters, kernelsize, activation = layers.LeakyReLU(alpha=0.1), padding = 'same'))\n",
    "model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model_1.add(layers.Conv2D(fourth_layer_filters, kernelsize, activation = layers.LeakyReLU(alpha=0.1), padding = 'same'))\n",
    "model_1.add(layers.Conv2D(fourth_layer_filters, kernelsize, activation = layers.LeakyReLU(alpha=0.1), padding = 'same'))\n",
    "model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model_1.add(layers.Flatten())\n",
    "\n",
    "model_1.add(layers.Dense(first_hidden_layer_nodes, activation = layers.LeakyReLU(alpha=0.1)))\n",
    "model_1.add(layers.Dropout(0.15))\n",
    "model_1.add(layers.Dense(second_hidden_layer_nodes, activation = layers.LeakyReLU(alpha=0.1)))\n",
    "model_1.add(layers.Dropout(0.15))\n",
    "model_1.add(layers.Dense(output_nodes, activation = 'softmax'))\n",
    "\n",
    "\n",
    "train_and_evaluate(model_1, image_train, label_train, image_val, label_val, epochs = 40, augment = {'horizontal_flip':  True, 'vertical_flip': True, 'rotation_range': 0.15, 'zoom_range': 0.15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ace847",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dae72",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-implement-major-architecture-innovations-for-convolutional-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7070c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Plankton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
